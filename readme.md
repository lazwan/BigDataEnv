# å¤§æ•°æ®ç¯å¢ƒæ­å»ºéƒ¨åˆ†

### æ³¨æ„ï¼š

- ç¯å¢ƒæ­å»ºæ‰€éœ€è¦çš„å®‰è£…åŒ…éœ€è¦æå‰ä½¿ç”¨ `xftp`ä¸Šä¼ ï¼Œæœ¬æ–‡æ¡£é»˜è®¤æ”¾åœ¨è™šæ‹Ÿæœºçš„`/root`ç›®å½•ä¸‹
- ç¯å¢ƒæ­å»ºæ‰€éœ€å®‰è£…åŒ… [ä¸‹è½½é“¾æ¥](https://pan.baidu.com/s/1vk1wVTdVxyY5wuD9Q1XUvg), æå–ç ï¼š`data`
- è¯·å­¦ä¼šä½¿ç”¨  `Tab` é”®è¿›è¡Œè¡¥å…¨æ–‡ä»¶è·¯å¾„

#### 1ã€è™šæ‹Ÿæœºå®‰è£…

è¿™é‡Œæˆ‘ä»¬é»˜è®¤ä½¿ç”¨çš„æ˜¯ `VMware 16` ï¼Œ`VMware 15`ä¹Ÿå¯ä»¥ï¼Œ`CentosOS`ç‰ˆæœ¬é€‰æ‹©çš„æ˜¯ `CentOS 7`

å…·ä½“å®‰è£…è¯·çœ‹ğŸ‘‰ [è™šæ‹Ÿæœºå®‰è£…CentOS 7](https://gitee.com/lazywa/BigData/blob/master/è™šæ‹Ÿæœºå®‰è£…CentOS7.md))

#### 2ã€å…‹éš†è™šæ‹Ÿæœºã€è¿æ¥ `Xshell`

è¿™é‡Œæˆ‘ä»¬çº¦å®šä¸‰å°è™šæ‹Ÿæœºåç§°é»˜è®¤ä¸º `master`ã€`slave1`ã€`slave2` (æ¯”èµ›ä¸­ä¼šæœ‰ä¸åŒçš„è¦æ±‚)

å…·ä½“å®‰è£…è¯·çœ‹ğŸ‘‰[è™šæ‹Ÿæœºå…‹éš†ã€è¿æ¥Xshell](https://gitee.com/lazywa/BigData/blob/master/è™šæ‹Ÿæœºå…‹éš†ã€è¿æ¥Xshell.md)

#### 3ã€å…³é—­é˜²ç«å¢™(ä¸‰å°è™šæ‹Ÿæœºéƒ½è¦æ“ä½œ)

è¯·æ ¹åŸºä½ çš„ç³»ç»Ÿç‰ˆæœ¬é€‰æ‹©å¯¹åº”çš„å‘½ä»¤

- Centos 7 å‘½ä»¤ (ä¸¤æ¡å‘½ä»¤åˆ†åˆ«æ‰§è¡Œ)

  ```shell
  systemctl stop firewalld.service
  systemctl disable firewalld.service
  ```

- Centos 6 å‘½ä»¤ (ä¸¤æ¡å‘½ä»¤åˆ†åˆ«æ‰§è¡Œ)

  ```shell
  service iptables stop
  chkconfig iptables off
  ```

#### 4ã€ä¿®æ”¹ hostsï¼Œæ”¹å®Œæ‹·è´åˆ°å¦å¤–ä¸¤å°æœºå™¨

å‘½ä»¤ï¼š

```shell
vim /etc/hosts
```

æ·»åŠ ä»¥ä¸‹å†…å®¹:

**è¯·æ³¨æ„æ³¨æ„ä¸€ä¸‹å‡ ç‚¹ï¼š**

- ä¸»æœºåä¸­**ä¸€å®šä¸€å®šä¸€å®š**ä¸èƒ½æœ‰ä¸‹åˆ’çº¿ã€è¿æ¥ç¬¦ï¼ï¼ï¼
- è¯·ä¸è¦å¤åˆ¶ä»¥ä¸‹å†…å®¹ç›´æ¥ç”¨ï¼Œéœ€è¦è®²`master_ip`æ”¹æˆå¯¹åº”ä¸»æœºçš„ IP åœ°å€(å…·ä½“è¯·çœ‹æœ€åçš„ç¤ºä¾‹)

- æ¯”èµ›æ—¶æ ¹æ®å®˜æ–¹è¦æ±‚ç»Ÿä¸€ä½¿ç”¨ `azy01slave1`, `azy01sla` ç±»çš„åå­—

```shell
master_ip	master
slave1_ip	slave1
slave2_ip	slave2
```

ç¤ºä¾‹ï¼š

![image-20201125235322838](image/image-20201125235322838.png)

æ‹·è´ `hosts` åˆ°å¦å¤–ä¸¤å°è™šæ‹Ÿæœº

å‘½ä»¤(è¯·**é€æ¡åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œ**)ï¼š

```shell
scp /etc/hosts slave1:/etc/hosts
scp /etc/hosts slave2:/etc/hosts
```

#### 5ã€é…ç½®å…å¯†ç™»å½•ï¼ˆåœ¨ master ä¸Šï¼‰

ç”Ÿæˆ SSH å…¬é’¥(éœ€è¦**æŒ‰å¤šæ¬¡å›è½¦**ï¼Œç›´åˆ°å‡ºç°ä¸€ä¸ªâ€œæ¡†â€)

```shell
ssh-keygen -t rsa
```

é…ç½®ä¸‰å°ä¸»æœºçš„å…å¯†ç™»å½•(è¯·**é€æ¡åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œ**)

```shell
ssh-copy-id -i master
ssh-copy-id -i slave2
ssh-copy-id -i slave1
```

#### 6ã€å®‰è£… JDK

1. è§£å‹ `jdk` å®‰è£…åŒ…

   ```shell
   tar -zxvf jdk-8u192-linux-x64.tar.gz
   ```

2. å°†è§£å‹å‡ºæ¥çš„æ–‡ä»¶å¤¹ `jdk1.8.0_192` ç§»åŠ¨åˆ° `/opt` ç›®å½•ä¸‹ï¼Œå¹¶ä¿®æ”¹æ–‡ä»¶å¤¹åç§°ä¸º `jdk`

   ```shell
   mv jdk1.8.0_192 /opt/jdk
   ```

3. é…ç½® `jdk` ç¯å¢ƒå˜é‡

   å‘½ä»¤ï¼š

   ```shell
   vim /etc/profile
   ```

   åœ¨æ³¨é‡Šçš„æœ€åä¸€è¡Œæ·»åŠ ä»¥ä¸‹å†…å®¹ (å¦‚ä¸‹å›¾)

   ```shell
   export JAVA_HOME=/opt/jdk
   export PATH=$JAVA_HOME/bin:$PATH
   ```

   ![image-20201125235224054](image/image-20201125235224054.png)

4. æ˜¾ç¤ºå½“å‰ç¯å¢ƒå˜é‡(å¯é€‰ï¼Œ ä½œç”¨ä¸ºå¤‡ä»½ PATHï¼Œé˜²æ­¢ PATH å˜é‡å—æŸ)

   ```shell
   echo $PATH
   ```

   æ­¤æ—¶ä¼šè¾“å‡ºä¸€ä¸ªç±»ä¼¼ä»¥ä¸‹çš„å†…å®¹

   ```shell
   /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin
   ```

   å¦‚æœç¯å¢ƒå˜é‡é…ç½®é”™äº†ï¼Œå¯ä»¥é€šè¿‡è¿™ä¸ªè¿›è¡Œæ¢å¤

   æ¢å¤å‘½ä»¤ï¼š

   ```shell
   export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin
   ```

5. ä½¿ç¯å¢ƒå˜é‡ç”Ÿæ•ˆ

   å‘½ä»¤ï¼š

   ```shell
   source /etc/profile
   ```

   è‹¥ `source`å `ls`ã€`cd`ç­‰æŒ‡ä»¤æ²¡æœ‰äº†ï¼Œå¯ä»¥é€šè¿‡ä¸Šä¸€æ­¥è¿›è¡Œæ¢å¤ï¼Œç„¶åé‡æ–°ä¿®æ”¹ `profile`æ–‡ä»¶

6. å¤åˆ¶ç¯å¢ƒå˜é‡åˆ°å¦å¤–ä¸¤å°æœºå™¨ä¸Š((è¯·**é€æ¡åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œ**)

   ```shell
   scp /etc/profile slave1:/etc
   scp /etc/profile slave2:/etc
   ```
   
7. å¤åˆ¶ `jdk` åˆ°å¦å¤–ä¸¤å°æœºå™¨ä¸Š((è¯·**é€æ¡åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œ**)

   ```shell
   scp -r /opt/jdk slave1:/opt
   scp -r /opt/jdk slave2:/opt
   ```

#### 7ã€`hadoop` é…ç½®

1. è§£å‹ `hadoop` å®‰è£…åŒ…

   ```shell
   tar -zxvf hadoop-2.7.6.tar.gz
   ```

2. å°†è§£å‹å‡ºæ¥çš„æ–‡ä»¶å¤¹ ` hadoop-2.7.6.` ç§»åŠ¨åˆ° `/opt` ç›®å½•ä¸‹ï¼Œå¹¶ä¿®æ”¹æ–‡ä»¶å¤¹åç§°ä¸º `jdk`

   ```shell
   mv hadoop-2.7.6 /opt/hadoop
   ```

3. é…ç½®`hadoop`ç¯å¢ƒå˜é‡

   å‘½ä»¤ï¼š`vim /etc/profile` (åœ¨åˆšåˆšé…ç½®çš„ `jdk` ç¯å¢ƒå˜é‡åæ·»åŠ å³å¯)

   ```shell
   export HADOOP_HOME=/opt/hadoop
   export PATH=$HADOOP_HOME/bin:$PATH
   ```
   ä½¿ç¯å¢ƒå˜é‡ç”Ÿæ•ˆ

   ```shell
   source /etc/profile
   ```

4. å¤åˆ¶ç¯å¢ƒå˜é‡åˆ°å¦å¤–ä¸¤å°æœºå™¨ä¸Š((è¯·**é€æ¡åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œ**)

   ```shell
   scp /etc/profile slave1:/etc
   scp /etc/profile slave2:/etc
   ```
   
5. ä¿®æ”¹ `hadoop` çš„é…ç½®æ–‡ä»¶

   1. è¿›å…¥ `hadoop` é…ç½®æ–‡ä»¶çš„æ–‡ä»¶å¤¹

      ```shell
      cd /opt/hadoop/etc/hadoop
      ```

   2. ä¿®æ”¹ `slaves `

      å‘½ä»¤ï¼š

      ```shell
      vim slaves
      ```

      åˆ é™¤é‡Œé¢çš„ `localhost`ï¼Œæ·»åŠ ä»¥ä¸‹å†…å®¹

      ```
      slave1
      slave2
      ```

   3. ä¿®æ”¹ `hadoop-env.sh`

      å‘½ä»¤ï¼š

      ```shell
      vim hadoop-env.sh
      ```

      å°† `JAVA_HOME` ä¿®æ”¹æˆ

      ```shell
      export JAVA_HOME=/opt/jdk
      ```

      ![image-20201126000041086](image/image-20201126000041086.png)

   4. éœ€æ”¹ `core-site.xml`

      å‘½ä»¤ï¼š

      ```shell
      vim core-site.xml
      ```

      æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼š(ä¸€å®šè¦åœ¨ <configuration> </configuration> ä¹‹é—´æ·»åŠ )

      ```xml
      <property>
          <name>fs.defaultFS</name>
          <value>hdfs://master:9000</value>
      </property>
      
      <property>
          <name>hadoop.tmp.dir</name>
          <value>/opt/hadoop/tmp</value>
      </property>
      ```

      ![image-20201126000325569](image/image-20201126000325569.png)

   5. ä¿®æ”¹ `hdfs-site.xml`

      å‘½ä»¤ï¼š

      ```shell
      vim hdfs-site.xml
      ```

      æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼š(ä¸€å®šè¦åœ¨ <configuration> </configuration> ä¹‹é—´æ·»åŠ )

      ```xml
      <property>
          <name>dfs.replication</name>
          <value>1</value>
      </property>
      ```

   6. ä¿®æ”¹ `mapred-site.xml`

      - ä» `mapred-site.xml.template`å¤åˆ¶å‡º `mapred-site.xml`

        å‘½ä»¤ï¼š

       ```shell
       cp mapred-site.xml.template mapred-site.xml
       ```

       - ç”¨ `vim ` ç¼–è¾‘

         å‘½ä»¤ï¼š

         ```
         vim mapred-site.xml
         ```

         æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼š(ä¸€å®šè¦åœ¨ <configuration> </configuration> ä¹‹é—´æ·»åŠ )

         ```xml
         <property>
             <name>mapreduce.framework.name</name>
             <value>yarn</value>
         </property>
         ```

   7. ä¿®æ”¹ `yarn-site.xml`

       å‘½ä»¤ï¼š

       ```shell
       vim yarn-site.xml
       ```

       æ·»åŠ ä»¥ä¸‹å†…å®¹ï¼š(ä¸€å®šè¦åœ¨ <configuration> </configuration> ä¹‹é—´æ·»åŠ )

       ```xml
       <property>
           <name>yarn.resourcemanager.hostname</name>
           <value>master</value>
       </property>

       <property>
           <name>yarn.nodemanager.aux-services</name>
           <value>mapreduce_shuffle</value>
       </property>
       ```

6. æŠŠ `hadoop` æ‹·åˆ°å…¶ä»–æœºå™¨ä¸Š(è¯·**é€æ¡åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œ**)

    ```shell
    scp -r /opt/hadoop slave1:/opt/
    scp -r /opt/hadoop slave2:/opt/
    ```

7. åœ¨ master ä¸Šåˆå§‹åŒ– `hadoop` é›†ç¾¤

    ```shell
    hadoop namenode -format
    ```

8. å¯åŠ¨èŠ‚ç‚¹

   - è¿›å…¥ `hadoop` çš„ `sbin` æ–‡ä»¶å¤¹

     ```shell
     cd /opt/hadoop/sbin
     ```

   - å¯åŠ¨ `hadoop`

     ```shell
     ./start-all.sh
     ```

9. ä½¿ç”¨ `jps` å‘½ä»¤æŸ¥çœ‹è¿›ç¨‹

   ```shell
   jps
   ```

   è‹¥ `master` æ˜¾ç¤º 

   ```shell
   Namenode
   Resourcemanager
   SecondaryNameNode
   Jps
   ```

   `slave1`ã€`slave2`æ˜¾ç¤ºï¼š

   ```shell
   Datanode
   Nodemanager
   Jps
   ```

   åˆ™å®‰è£…æˆåŠŸï¼Œå¦åˆ™å®‰è£…å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°æ­¥éª¤æˆ–è€…é…ç½®æ–‡ä»¶æ˜¯å¦å‡ºé”™

#### 8ã€MySQL

1. å› ä¸º `CentOS 7` é»˜è®¤å®‰è£…äº† `mariadb-libs` ä¼šå¯¼è‡´å®‰è£…ä¸ä¸Š `MySQL` æ‰€ä»¥å…ˆå¸è½½å†²çªæº

    ```shell
    rpm -e --nodeps mariadb-libs
    ```
    
2. ä½¿ç”¨ `rpm` åŒ…å®‰è£…(å…ˆå®‰è£… `MySQL-client` å†å®‰è£… `MySQL-server`ï¼Œä¸€æ¡æ¡æ‰§è¡Œ)

    ```shell
    rpm -ivh MySQL-client-5.1.73-1.glibc23.x86_64.rpm
    rpm -ivh MySQL-server-5.1.73-1.glibc23.x86_64.rpm
    ```

3. å¯åŠ¨ `mysql` æœåŠ¡(å®‰è£…å¥½ `server` åä¸€èˆ¬ä¼šè‡ªå¯åŠ¨ï¼Œä¸éœ€è¦æ‰‹åŠ¨å¯åŠ¨ï¼Œå¯ä»¥çœç•¥)

    ```shell
    service mysql start
    ```

4. åŠ å…¥åˆ°å¼€æœºå¯åŠ¨é¡¹

    ```shell
    chkconfig mysql on
    ```

5. åˆå§‹åŒ–é…ç½® `mysql` æœåŠ¡

    ```shell
    mysql_secure_installation
    ```

    - `Enter current password for root (enter for none):` ï¼šç›´æ¥æŒ‰å›è½¦

    - `Set root password? [Y/n]`ï¼šè¾“å…¥ `Y`
    - `New password:`ï¼š è¾“å…¥ `123456`
    - `Re-enter new password:`ï¼šå†è¾“å…¥ `123456`
    - åé¢å…¨éƒ¨å›è½¦å³å¯

6. ç™»å½• MySQL(å¯†ç æ˜¯ï¼š`123456`)

    ```shell
    mysql -uroot -p
    ```

7. è®¾ç½®ç”¨æˆ·æƒé™(è¯·**é€æ¡åœ¨ SQL å‘½ä»¤è¡Œä¸­è¿è¡Œ**)

    ```sql
    use mysql;
    update user set host='%' where user = 'root';
    flush privileges;
    ```

9. å»º `hive` è¡¨

   ```sql
   create database hive default charset utf8;
   ```

10. æ£€æŸ¥è¡¨æ˜¯å¦åˆ›å»ºæˆåŠŸ

    ```sql
    show databases;
    ```

11. é€€å‡º `MySQL`

    ```
    exit;
    ```

12. `MySQL` é…ç½®çš„ç–‘éš¾è§£ç­”

- æ£€æŸ¥ `service mysql status`ï¼Œå¦‚æœåœ¨éå¯åŠ¨çŠ¶æ€æœ‰é”ä½ï¼Œç›´æ¥åˆ å»é”æ–‡ä»¶ï¼ˆ`status` ä¸Šä¼šæŒ‡å®šè·¯å¾„ï¼‰ã€‚

- `mysql` å¯èƒ½ä¼šå‡ºç°å¯åŠ¨ä¸å®Œå…¨çš„æƒ…å†µã€‚`ps -aux | ps -ef` æ£€æŸ¥æ‰€æœ‰ `mysql` æœåŠ¡çš„è¿›ç¨‹å·ï¼Œ`kill -9` æ€æ­» `mysql` çš„æ‰€æœ‰è¿›ç¨‹é‡æ–°å¯åŠ¨ã€‚

#### 9ã€hive é…ç½®æ–‡ä»¶

1. `hive-env.sh`ï¼ˆ`hive-env.sh.template` -> `hive-env.sh`ï¼‰

   ```sh
   HADOOP_HOME=/opt/hadoop
   JAVA_HOME=/opt/jdk
   HIVE_HOME=/opt/hive
   ```

2. `hive-site.xml`ï¼ˆä» `hive-default.xml.template` æ‹·è´ï¼‰

   ```
   " vim é…ç½®æ–¹ä¾¿æŸ¥æ‰¾ï¼ˆé€‰æ‹©é…ç½®ï¼‰
   set ignorecase " è‡ªåŠ¨è·³åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…çš„ç»“æœ
   set incsearch  " æœç´¢æ—¶å¿½ç•¥å¤§å°å†™
   ```

   ```xml
   <!-- vimä½¿ç”¨`/`åŠ å…³é”®å­—æœç´¢ï¼Œnåˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæœç´¢é¡¹ï¼ŒNåˆ‡æ¢åˆ°ä¸Šä¸€ä¸ªæœç´¢é¡¹ -->
   <property>
      <name>javax.jdo.option.ConnectionURL </name> 
      <value>jdbc:mysql://ä¸»æœºip(ä¸è¦ä½¿ç”¨hosts):3306/hive?useSSL=false</value> 
   </property>
   
   <property> 
      <name>javax.jdo.option.ConnectionDriverName </name> 
      <value>com.mysql.jdbc.Driver </value> 
   </property> 
   
   <property>
      <name>javax.jdo.option.ConnectionUserName</name>
      <value>root</value>
   </property> 
   
   <property> 
      <name>javax.jdo.option.ConnectionPassword </name> 
      <value>123456</value> 
   </property>
   
   <property>
      <name>hive.querylog.location</name>
   <value>/opt/hive/tmp</value>
   </property>
   
   <property>
      <name>hive.exec.local.scratchdir</name>
   <value>/opt/hive/tmp</value>
   </property>
   
   <property>
       <name>hive.downloaded.resources.dir</name>
       <value>/opt/hive/tmp</value>
   </property>
   ```

```shell
# æä¾› mysql-connector jaråŒ…
cp /root/mysql-connector-java-5.1.17-bin.jar /opt/hive/lib/

# æ›¿æ¢æ‰ hadoop çš„ jline çš„ç‰ˆæœ¬ (é«˜ç‰ˆæœ¬çš„ä¼¼ä¹ä¸è‡ªå¸¦ jline, ç›®å‰åªçŸ¥é“ hadoop1.6 éœ€è¦å…ˆåˆ é™¤ jline-0.9.94.jar)
rm -rf /opt/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar
cp /opt/hive/lib/jline-2.12.jar /opt/hadoop/share/hadoop/yarn/lib/
```

**æ‹·è´é…ç½®æ–‡ä»¶åˆ°æ‰€æœ‰æœºå™¨ä¸Š**

```shell
scp -r /opt/hive slave1:/opt
scp -r /opt/hive slave2:/opt
```

**åˆå§‹åŒ–å…ƒæ•°æ®ï¼š**

```shell
schematool -dbType mysql -initSchema
```

**å¯åŠ¨ hiveï¼ŒæŸ¥çœ‹æ˜¯å¦è¿è¡Œæ­£å¸¸**

```sql
show databases;
set hive.cli.print.current.db=true;
exit;
```

**hadoop é€€å‡º safe mode**

```bash
hadoop dfsadmin -safemode leave
```

**hdfs-site.xmlçš„ä¸€äº›é…ç½®**

```
è®¾ç½®dfsæƒé™æ‰“å¼€ 				truedfs.permissions
è®¾ç½®HDFSæ•°æ®å—çš„å¤‡ä»½æ•°		  dfs.replication 
è®¾ç½®æ•°æ®å—å†™å…¥çš„æœ€å¤šé‡è¯•æ¬¡æ•° 	   dfs.client.block.write.retries
è®¾ç½®dfsæœ€å¤§å¹¶å‘å¯¹è±¡æ•°          dfs.max.objects
è®¾ç½®DateNodeå¯åŠ¨çš„æœåŠ¡çº¿ç¨‹æ•°   dfs.datanode.handler.count
```

**hdfs dfsçš„ä¸€äº›æŒ‡ä»¤ï¼Œå…¶å®è·Ÿ bash çš„æŒ‡ä»¤å·®ä¸å¤šï¼š**

```shell
-ls
-mkdir [-p]
-touchz  (è·Ÿbashçš„touchæ˜¯ä¸€æ ·çš„)
-rmr
-appendToFile File1 File2   (è¿½åŠ File1åˆ°File2å°¾éƒ¨)
-chmod 644 File1
-cat
-put

# ä¸Šä¼ å‘½ä»¤ï¼šhdfs dfs -put æœ¬åœ°æ–‡ä»¶è·¯å¾„ hdfsçš„è·¯å¾„
hdfs dfs -put /usr/local/testdata/anhui.txt /data/

# ä¸‹è½½å‘½ä»¤ï¼šä¸Šä¼ å‘½ä»¤ï¼šhdfs dfs -get  hdfsçš„è·¯å¾„ æœ¬åœ°æ–‡ä»¶è·¯å¾„
hdfs dfs -get /data/anhui.txt /usr/local/
```

<p style="color:grey">HDFS ä¼ è¾“é—®é¢˜ä¼˜å…ˆè€ƒè™‘é˜²ç«å¢™çš„é—®é¢˜ï¼Œä¼˜å…ˆå…ˆå°è¯•å…³é—­é˜²ç«å¢™ï¼ˆä½†å®é™…æ¯”èµ›ç¯å¢ƒå¥½åƒæ²¡æœ‰é˜²ç«å¢™ï¼‰</p>

HDFS æŠ¥é”™ï¼š  
`appendToFile: Failed to APPEND_FILE /data/file/data1.csv for DFSClient_NONMAPREDUCE_-1657827142_1 on 192.168.1.100 because lease recovery is in progress. Try again later.`
åœ¨ `hdfs-site.xml` ä¸­è¿½åŠ  `name: dfs.client.block.write.replace-datanode-on-failure.policy value=NEVER`

#### 10ã€zookeeper

**é…ç½®ç¯å¢ƒå˜é‡**

```shell
ZOOKEEPER_HOME=/opt/zookeeper
export PATH=$ZOOKEEPER_HOME/bin:$PATH
```

**ä¿®æ”¹é…ç½®æ–‡ä»¶**

`zoo.cfg`ï¼ˆä» `zoo_sample.cfg` å¤åˆ¶ï¼‰

```cfg
dataDir=/opt/zookeeper/data
server.0=master:2888:3888
server.1=slave1:2888:3888
server.2=slave2:2888:3888
```

**åŒæ­¥åˆ°å…¶å®ƒèŠ‚ç‚¹**

```shell
scp -r /opt/zookeeper slave1:/opt
scp -r /opt/zookeeper slave2:/opt
```

**åˆ›å»º `/opt/zookeepe/data` ç›®å½•(æ¯å°æœºå™¨éƒ½éœ€è¦é…ç½®)**

```shell
mkdir /opt/zookeeper/data

# åœ¨ data ç›®å½•ä¸‹åˆ›å»º myid æ–‡ä»¶
vim myid
# åˆ†åˆ«åŠ ä¸Š0, 1, 2
# master -> 0
# slave1  -> 1
# slave2  -> 2
```

**å¯åŠ¨ zookeeper**

```shell
# ä¸‰å°éƒ½éœ€è¦æ‰§è¡Œ
zkServer.sh start

# æŸ¥çœ‹çŠ¶æ€
zkServer.sh status

# å½“æœ‰ä¸€ä¸ª leader çš„æ—¶å€™å¯åŠ¨æˆåŠŸ

# è¿æ¥ zookeeper
zkCli.sh

# zk shell æ“ä½œ
ls /                  æŸ¥æ‰¾æ ¹ç›®å½•
create /test abc      åˆ›å»ºèŠ‚ç‚¹å¹¶èµ‹å€¼
get /test             è·å–æŒ‡å®šèŠ‚ç‚¹çš„å€¼
set /test cb          è®¾ç½®å·²å­˜åœ¨èŠ‚ç‚¹çš„å€¼
rmr /test             é€’å½’åˆ é™¤èŠ‚ç‚¹
delete /test/test01   åˆ é™¤ä¸å­˜åœ¨å­èŠ‚ç‚¹çš„èŠ‚ç‚¹
```

#### 11ã€hbase é…ç½®æ–‡ä»¶

1. hbase-env.sh

   ```sh
   # 1ã€æ”¹JAVA_HOME
   
   # 2.1ã€æœªå®‰è£… zookeeper
   	export HBASE_MANAGES_ZK=true
   # 2.2ã€å®‰è£… zookeeper
   	export HBASE_MANAGES_ZK=false
   # è¿™ä¸ªå…³ç³»åˆ° HMaster çš„å¯åŠ¨
   ```

2. hbase-site.xml

   ```xml
   <property> 
   	<name>hbase.rootdir</name>
       <value>hdfs://matser/hbase</value>
   </property>
   
   <property> 
       <name>hbase.cluster.distributed</name>
       <value>true</value> 
   </property> 
   
   <property> 
       <name>hbase.zookeeper.quorum</name>
       <value>master,slave1,slave2</value>
   </property>
   
   # è£…äº† zookeeper åˆ™ä¸éœ€è¦æ­¤é…ç½®
   <property>
      <name>hbase.zookeeper.property.dataDir</name> 
      <value>/opt/hbase/zookeeper</value>
   </property>
   ```

3. regionservers

   ```
   slave1
   slave2
   ```

**æ‹·è´é…ç½®æ–‡ä»¶åˆ°æ‰€æœ‰æœºå™¨ä¸Š**

```shell
scp -r /opt/hbase node1:/opt
scp -r /opt/hbase node2:/opt
```

**å¯åŠ¨ hbase**

```shell
./start-hbase.sh

# è¿›å…¥ hbase sell
hbase shell
```

åœ¨masterã€slave2ã€slave3ä¸­çš„ä»»æ„ä¸€å°æœºå™¨ä½¿ç”¨`./bin/hbase shell `è¿›å…¥hbaseè‡ªå¸¦çš„shellç¯å¢ƒï¼Œç„¶åä½¿ç”¨å‘½ä»¤`version`ç­‰ï¼Œè¿›è¡ŒæŸ¥çœ‹hbaseä¿¡æ¯åŠå»ºç«‹è¡¨ç­‰æ“ä½œ

**å…³é—­å®‰å…¨æ¨¡å¼**

```shell
hadoop dfsadmin -safemode leave
```

ä¸»èŠ‚ç‚¹: 

1. HMaster

2. HQuorumPeer

å­èŠ‚ç‚¹: 

1. HRegionServer

2. HQuorumPeer

**hadoopé…ç½®çš„ç–‘éš¾è§£ç­”**

*namenode æ²¡æœ‰å¯åŠ¨æˆåŠŸï¼š*

æŸ¥çœ‹ namenode çš„æ—¥å¿—ã€‚æ ¹æ®å®é™…æƒ…å†µéšæœºåº”å˜ã€‚

å¤§éƒ¨åˆ†æƒ…å†µå°è¯•åˆ é™¤ hadoop çš„ tmp ç›®å½•ï¼Œè§£å†³ namenode å¯åŠ¨æ•…éšœã€‚

ä»èŠ‚ç‚¹çš„ NodeManager æ²¡æœ‰å¯åŠ¨ï¼Œå°è¯•å°† hadoop çš„é…ç½®æ–‡ä»¶æ‹·å‡ºï¼Œé‡æ–°è§£å‹å®‰è£… hadoop é‡æ–°åˆå§‹åŒ–ã€‚

**æ³¨æ„ hadoop æ‹·è´åˆ°å…¶ä»–ä»èŠ‚ç‚¹åœ¨åˆå§‹åŒ– namenode ä¹‹å‰ã€‚**

#### 12ã€Spark

**é…ç½®ç¯å¢ƒå˜é‡**

```shell
export SPARK_HOME=/opt/spark
export PATH=$SPARK_HOME/bin:$PATH
```

**ä¿®æ”¹é…ç½®æ–‡ä»¶**  
1. **`spark-env.sh`(ä» `spark-env.sh.template` å¤åˆ¶)**

   ```shell
   export SPARK_MASTER_IP=master
   export SPARK_MASTER_PORT=7077
   export SPARK_WORKER_CORES=1  // æ ¹æ®è™šæ‹Ÿæœºæƒ…å†µè®¾ç½®
   export SPARK_WORKER_INSTANCES=1
   export SPARK_WORKER_MEMORY=1g // æ ¹æ®è™šæ‹Ÿæœºæƒ…å†µè®¾ç½®
   export HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
   export JAVA_HOME=/opt/jdk
   ```

2. **`slaves`(ä» `slaves.template` å¤åˆ¶)**

   ```
   slave1
   slave2
   ```

**æ‹·è´é…ç½®æ–‡ä»¶åˆ°æ‰€æœ‰æœºå™¨ä¸Š**

```shell
scp -r /opt/spark slave1:/opt
scp -r /opt/spark slave2:/opt
```

**å¯åŠ¨ Spark**

```shell
./sbin/start-all.sh

# è®¿é—® Spark UI
http://master:8080
```

**å¾€ yarn æäº¤ä»»åŠ¡éœ€è¦å¢åŠ ä¸¤ä¸ªé…ç½®(`/opt/hadoop/etc/hadoop/yarn-site.xml`)**

```xml
<property>
    <name>yarn.nodemanager.pmem-check-enabled</name>
    <value>false</value>
</property>

<property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
</property>
```

**åŒæ­¥åˆ°å…¶ä»–èŠ‚ç‚¹ï¼Œé‡å¯ yarn**

```shell
scp -r /opt/hadoop/etc/hadoop/yarn-site.xml slave1:`pwd`
scp -r /opt/hadoop/etc/hadoop/yarn-site.xml slave2:`pwd`

/opt/hadoop/sbin/stop-yarn.sh
/opt/hadoop/sbin/start-yarn.sh
```

#### 13ã€Python

**å®‰è£…ç¼–è¯‘æ‰€éœ€çš„ç¯å¢ƒ(åœ¨`tensorflow_torch.tgz`å†…)**

```shell
tar -zxvf tensorflow_torch.tgz
cd tensorflow_torch/rpm
rpm -ivh --nodeps --force *.rpm
```

**ç¼–è¯‘å®‰è£…**

```shell
tar -zxvf Python-3.6.3.tgz
cd Python-3.6.3
./configure --prefix=/opt/python36

make

# make ç»“æŸåè¿›è¡Œ
make install
```

**åˆ›å»ºè½¯é“¾æ¥**

```shell
ln -s /opt/python36/bin/python3 /usr/bin/python3
ln -s /opt/python36/bin/pip3 /usr/bin/pip3
```

**å‡çº§ pip**

```shell
cd /root/tensorflow_torch
pip3 install pip-20.2.3-py2.py3-none-any.whl
```

**å®‰è£… Tensorflow**

```shell
# numpy-1.17.2-cp36-cp36m-manylinux1_x86_64.whl
# protobuf-3.9.2-cp36-cp36m-manylinux1_x86_64.whl
# requirements.txt
# six-1.12.0-py2.py3-none-any.whl
# tensorflow-1.1.0rc1-cp36-cp36m-manylinux1_x86_64.whl
# Werkzeug-0.16.0-py2.py3-none-any.whl
# wheel-0.33.6-py2.py3-none-any.whl
cd /root/tensorflow_torch/tensorflow
pip3 install *.whl
```

**Tensorflow æµ‹è¯•ä»£ç **

```python
import tensorflow as tf
sess = tf.Session()
hello = tf.constant('Hello,world!')
print(sess.run(hello))
```

**å®‰è£… PyTorch**

```shell
cd /root/tensorflow_torch/pytorch
# å®‰è£… future(è¦éœ€è¦å…ˆå®‰è£…ï¼Œä¸ç„¶åé¢ä¼šæŠ¥é”™)
tar zxvf future-0.18.2.tar.gz
cd future-0.18.2
python3 setup.py install

# å®‰è£…å…¶ä»–
# Pillow-7.2.0-cp36-cp36m-manylinux1_x86_64.whl
# torch-1.6.0+cpu-cp36-cp36m-linux_x86_64.whl
# torchvision-0.7.0+cpu-cp36-cp36m-linux_x86_64.whl
cd ..
pip3 install *.whl
```

**PyTorch æµ‹è¯•ä»£ç **

```python
import torch
print(torch.__version__)
print(torch.tensor([1, 2]))
```

**æ’é”™**

```shell
python3.6: error while loading shared libraries: libpython3.6m.so.1.0:cannot open shared object file: No such file or directory

# ä½¿ç”¨å‘½ä»¤ldd /usr/local/Python-3.6/bin/python3 æ£€æŸ¥å…¶åŠ¨æ€é“¾æ¥
```

```shell
# æ‹·è´æ–‡ä»¶åˆ°libåº“
cd /root/Python-3.6.5
cp libpython3.6m.so.1.0 /usr/local/lib64/
cp libpython3.6m.so.1.0 /usr/lib/
cp libpython3.6m.so.1.0 /usr/lib64/
```

**æœ‰ç½‘ç»œæƒ…å†µä¸‹çš„å®‰è£…**

```
yum install python3
pip3 install --upgrade pip
pip3 install tensorflow
pip3 install torch
```

**ä¿®æ”¹ pip æº**

pip é»˜è®¤æºä¸‹è½½å¾ˆæ…¢æ‰€ä»¥å»ºè®®ä¿®æ”¹æˆå›½å†…é•œåƒæº

1. æ‰‹åŠ¨ä¿®æ”¹

   - åœ¨ `~/`ç›®å½•ä¸‹æ–°å»º `.pip`æ–‡ä»¶å¤¹: 

   ```skell
   mkdir ~/.pip
   ```

   - åœ¨ `~/.pip`æ–‡ä»¶å¤¹ä¸‹æ–°å»º `pip.conf`å†™å…¥ä»¥ä¸‹å†…å®¹ï¼š`vim ~/.pip/pip.conf`

   ```
   [global]
   index-url = http://pypi.douban.com/simple/
   [install]
   trusted-host = pypi.douban.com
   ```

2. ä½¿ç”¨ `pqi` ä¿®æ”¹

   ```shell
   pip3 install pqi
   
   pqi ls
   pypi 	 https://pypi.python.org/simple/
   tuna 	 https://pypi.tuna.tsinghua.edu.cn/simple
   douban 	 http://pypi.douban.com/simple/
   aliyun 	 https://mirrors.aliyun.com/pypi/simple/
   ustc 	 https://mirrors.ustc.edu.cn/pypi/web/simple
   
   pqi use <name> # <name> ä¸ºä»¥ä¸Šæ˜¾ç¤ºæºçš„åç§°ï¼Œå»ºè®®ä½¿ç”¨ ustc æˆ– douban
   ```

   
